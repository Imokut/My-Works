{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt('file_name.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"file_name.csv\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmaxscaler, standardscalar, normalizer\n",
    "    \"\"\"Use \n",
    "    minmaxscaler - if you have many outliers and you want to preserve the distribution\n",
    "    standardscaler -  to reshape to a normal/gaussian distribution\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "get_names = df.columns\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=get_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=get_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "norm_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Values\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation with extension\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train[col + 'was_missing'] = X_train[col].isnull()\n",
    "    imputed_X_valid[col + 'was_missing'] = X_valid[col].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Categorical column\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Discrete Features\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_train, X_valid = X_train.align(X_valid, join=\"left\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "X_train = pd.get_dummies(df.categorical, prefix='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "oh_cols_train = pd.DataFrame(oh_encoder.fit_transform(X_train[object_cols]))\n",
    "oh_cols_train.index = X_train.index\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "oh_X_train = pd.concat([num_X_train, oh_cols_train], axis=1)\n",
    "\n",
    "oh_cols_valid = pd.DataFrame(oh_encoder.transform(X_valid[object_cols]))\n",
    "oh_cols_valid.index = X_valid.index\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "oh_X_valid = pd.concat([num_X_valid, oh_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding\n",
    "#for categories more than 10\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Selecting good labels\n",
    "good_label_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "bad_label_cols = list(set(object_cols) - set(good_label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Low cardinality\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique()<10]\n",
    "high_cardinality_cols = list(set(object_cols) - set(low_cardinality_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "X_train[object_cols] = enc.fit_transform(X_train[object_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X, y = SMOTE(sampling_strategy=1, random_state=0).fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create cluster feature\n",
    "kmeans = KMeans(n_clusters=6, n_init=10, random_state=0)\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "X.head()\n",
    "\n",
    "X_cd = kmeans.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# Label features and join to dataset\n",
    "X_cd = pd.DataFrame(X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\n",
    "X = X.join(X_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create principal components\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # transpose the matrix of loadings\n",
    "    columns=component_names,  # so the columns are the principal components\n",
    "    index=X.columns,  # and the rows are the original features\n",
    ")\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.feature_engineering_new.ex5 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/ames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop('Rating')\n",
    "\n",
    "X_encode = X.sample(frac=0.25)\n",
    "y_encode = y[X_encode.index]\n",
    "X_pretrain = X.drop(X_encode.index)\n",
    "y_train = y[X_pretrain.index]\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Create the encoder instance. Choose m to control noise.\n",
    "encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "\n",
    "# Fit the encoder on the encoding split.\n",
    "encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# Encode the Zipcode column to create the final training data\n",
    "X_train = encoder.transform(X_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and X_train[cname].dtype=='object']\n",
    "numerical_cols = [cname for cname in X_train.columns if X_train.dtype in ['int64', 'float64']]\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps= [\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate = 0.001\n",
    "    n_jobs = 4\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "my_model.fit(X_train, y_train, early_stopping_rounds = 5, eval_set=[(X_valid, y_valid)], verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset (for demonstration, you can replace this with your dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "# Define hyperparameters grid for Grid Search\n",
    "param_grid = {\n",
    "    'alpha': [0, 0.1, 0.5, 1, 5],  # L1 regularization\n",
    "    'lambda': [0, 0.1, 0.5, 1, 5],  # L2 regularization\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3, scoring='accuracy', verbose = 2)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and corresponding accuracy score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Accuracy Score:\", best_score)\n",
    "\n",
    "# Train XGBoost classifier with the best hyperparameters\n",
    "best_xgb_clf = XGBClassifier(**best_params)\n",
    "best_xgb_clf.fit(X_train, y_train,\n",
    "    early_stopping_rounds=5,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    verbose=True)\n",
    "\n",
    "# Evaluate the trained model on the test set\n",
    "test_accuracy = best_xgb_clf.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load dataset (for demonstration, you can replace this with your dataset)\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'alpha': [0, 0.1, 0.5],  # L1 regularization\n",
    "    'lambda': [0, 0.1, 0.5],  # L2 regularization\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_reg = XGBRegressor(n_jobs=4, random_state=1)\n",
    "\n",
    "# Define the GridSearchCV with early stopping\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Add early stopping to the fit parameters\n",
    "fit_params = {\n",
    "    'early_stopping_rounds': 5,\n",
    "    'eval_set': [(X_valid, y_valid)],\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Fit the grid search with early stopping\n",
    "grid_search.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best estimator and evaluate on the validation set\n",
    "best_xgb_reg = grid_search.best_estimator_\n",
    "y_pred = best_xgb_reg.predict(X_valid)\n",
    "\n",
    "# Calculate R² score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "print(\"R² Score on Validation Set:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning on a metric with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=3, scoring='r2')\n",
    "\n",
    "my_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.001,\n",
    "    n_jobs=4,\n",
    "    random_state=1,\n",
    "    eval_metric='r2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search on preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membarked\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Applying SimpleImputer and will search for different scalers using GridSearchCV\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m numerical_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer()),\n\u001b[0;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Applying SimpleImputer and then OneHotEncoder\u001b[39;00m\n\u001b[0;32m     13\u001b[0m categorical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer()),\n\u001b[0;32m     15\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m))])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# numerical features from the dataset\n",
    "numerical_features = ['age', 'fare']\n",
    "\n",
    "# categorical features from the dataset\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "# Applying SimpleImputer and will search for different scalers using GridSearchCV\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', 'passthrough')])\n",
    "\n",
    "# Applying SimpleImputer and then OneHotEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "data_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numerical_transformer, numerical_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Creating preprocessor pipeline which will first transform the data\n",
    "# and then apply PCA.\n",
    "preprocessor = Pipeline(steps=[('data_transformer', data_transformer),\n",
    "                             ('reduce_dim',PCA())])\n",
    "\n",
    "\n",
    "# we are using Logistics Regression here\n",
    "classifier = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(random_state=0, max_iter=10000))])\n",
    "\n",
    "# We can utilize params grid to check for best hyperparameters or transformers\n",
    "# The syntax here is pipeline_step_name__parameters and we need to chain if we have nested pipelines \n",
    "param_grid = {\n",
    "    'preprocessor__data_transformer__numerical__imputer__strategy': ['mean', 'median'],\n",
    "    'preprocessor__data_transformer__categorical__imputer__strategy': ['constant','most_frequent'],\n",
    "    'preprocessor__data_transformer__numerical__scaler': [StandardScaler(), RobustScaler(), \\\n",
    "                                                          MinMaxScaler()],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "    'preprocessor__reduce_dim__n_components': [2, 5, 10],\n",
    "    'classifier__solver': ['liblinear','newton-cg', 'lbfgs','sag','saga']\n",
    "}\n",
    "\n",
    "# Doing a Grid Search\n",
    "grid_search = GridSearchCV(classifier, param_grid=param_grid)\n",
    "\n",
    "# fitting on our dataset\n",
    "grid_search.fit(X_train, y_train); # Semicolon to not print estimator in notebook\n",
    "\n",
    "\n",
    "\n",
    "# imports \n",
    "from sklearn import set_config                      # to change the display\n",
    "from sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\n",
    "\n",
    "# set config to diagram for visualizing the pipelines/composite estimators\n",
    "set_config(display='diagram')\n",
    "\n",
    "# Lets visualize the best estimator from grid search.\n",
    "grid_search.best_estimator_\n",
    "\n",
    "# saving pipeline as html format\n",
    "with open('titanic_data_pipeline_estimator.html', 'w') as f:  \n",
    "    f.write(estimator_html_repr(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization\n",
    "\n",
    "xgb_clf_l1 with L1 regularization by setting alpha=1 and reg_lambda=0.\n",
    "\n",
    "xgb_clf_l2 with L2 regularization by setting alpha=0 and reg_lambda=1.\n",
    "\n",
    "xgb_clf_l1_l2 with both L1 and L2 regularization by setting alpha=1 and reg_lambda=1.\n",
    "\n",
    "\n",
    "xgb_reg_l1 with L1 regularization by setting alpha=1 and lambda=0.\n",
    "xgb_reg_l2 with L2 regularization by setting alpha=0 and lambda=1.\n",
    "xgb_reg_l1_l2 with both L1 and L2 regularization by setting alpha=1 and lambda=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_l1 = XGBClassifier(objective='multi:softmax', num_class=3, alpha=1)\n",
    "xgb_clf_l2 = XGBClassifier(objective='multi:softmax', num_class=3, reg_lambda=1)\n",
    "xgb_clf_l1_l2 = XGBClassifier(objective='multi:softmax', num_class=3, alpha=1, reg_lambda=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg_l1 = XGBRegressor(alpha=1, lambda=0)\n",
    "xgb_reg_l2 = XGBRegressor(alpha=0, lambda=1)\n",
    "xgb_reg_l1_l2 = XGBRegressor(alpha=1, lambda=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Load dataset (for demonstration, you can replace this with your dataset)\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "xgb_reg = XGBRegressor()\n",
    "\n",
    "# Define hyperparameters distribution for Random Search\n",
    "param_dist = {\n",
    "    'alpha': uniform(0, 5),  # L1 regularization\n",
    "    'lambda': uniform(0, 5),  # L2 regularization\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'n_estimators': [50, 100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize Random Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_reg, param_distributions=param_dist, n_iter=50, cv=3, scoring='r2', random_state=42)\n",
    "\n",
    "# Perform Random Search to find the best hyperparameters\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train XGBoost regressor with the best hyperparameters\n",
    "best_xgb_reg = XGBRegressor(**best_params)\n",
    "best_xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_xgb_reg.predict(X_test)\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R² Score on Test Set:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Accuracy Score\n",
    "\n",
    "knn.score(X_test, y_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error:\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2]\n",
    "mean_absolute_error(y_true, y_predict)\n",
    "\n",
    "\n",
    "# Mean Squared Error:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_predict)\n",
    "\n",
    "\n",
    "# R² Score\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homogeneity:\n",
    "\n",
    "from sklearn.metrics import homogeneity_score\n",
    "homogeneity_score(y_true, y_predict)\n",
    "\n",
    "\n",
    "# V-measure:\n",
    "\n",
    "from sklearn.metrics import v_measure_score\n",
    "metrics.v_measure_score(y_true, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation:\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(knn, X_train, y_train, cv=4))\n",
    "print(cross_val_score(new_lr, X, y, cv=2))\n",
    "\n",
    "scores = -1 * cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Data Collection and Exploration\n",
    "\n",
    "Load your data and perform initial exploration (e.g., summary statistics, data types, missing values, class distribution).\n",
    "\n",
    "2. Data Preprocessing\n",
    "\n",
    "Handle Missing Values: Impute missing values using methods like mean/mode/median imputation or more advanced techniques like k-NN imputation.\n",
    "Category Encoding: Encode categorical variables using techniques like one-hot encoding or target encoding (for high cardinality features).\n",
    "Standardization: Standardize numerical features to have zero mean and unit variance, especially if algorithms like SVM, KNN, or PCA are used.\n",
    "\n",
    "3. Train-Test Split\n",
    "\n",
    "Split your data into training and testing sets (e.g., 70-30 or 80-20) to evaluate the model's performance on unseen data.\n",
    "\n",
    "4. Feature Engineering\n",
    "\n",
    "Mutual Information: Assess the importance of features and select the most relevant ones based on mutual information scores.\n",
    "Clustering for New Features: Use clustering algorithms (e.g., K-means) to create new features that capture the inherent structure of the data.\n",
    "Principal Component Analysis (PCA): Reduce dimensionality and remove multicollinearity by transforming features into principal components.\n",
    "\n",
    "5. Handling Imbalanced Data\n",
    "\n",
    "Oversampling: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to balance the class distribution in the training data.\n",
    "\n",
    "6. Model Training\n",
    "\n",
    "L1/L2 Regularization: Incorporate regularization to prevent overfitting and improve model generalization. For example, L1 for feature selection (Lasso) and L2 for ridge regression.\n",
    "Using XGBoost: Train an XGBoost model, which is robust and handles various types of data well.\n",
    "\n",
    "7. Hyperparameter Tuning\n",
    "\n",
    "Grid Search/Random Search: Optimize hyperparameters using Grid Search or Random Search with cross-validation to find the best model parameters.\n",
    "Cross-Validation: Use k-fold cross-validation to evaluate the model performance and ensure it generalizes well to unseen data.\n",
    "\n",
    "8. Model Evaluation\n",
    "\n",
    "Scoring: Assess the model using appropriate metrics (e.g., accuracy, precision, recall, F1-score, ROC-AUC) based on the problem context.\n",
    "\n",
    "9. Model Interpretation and Deployment\n",
    "\n",
    "Interpret the model results, understand feature importance, and deploy the model for production use if it meets the desired performance criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('your_data.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline for preprocessing\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Compute mutual information\n",
    "mi = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "X_train['cluster'] = kmeans.fit_predict(X_train)\n",
    "X_test['cluster'] = kmeans.predict(X_test)\n",
    "\n",
    "# Model Training and Hyperparameter Tuning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Define XGBoost model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Define hyperparameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "best_model = grid_search.best_estimator_\n",
    "scores = cross_val_score(best_model, X_train_pca, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross-validation accuracy:', scores.mean())\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_score = best_model.score(X_test_pca, y_test)\n",
    "print('Test set accuracy:', test_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
